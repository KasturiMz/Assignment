{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243e3ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\\nrepresent?\\n-> R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines \\nthe proportion of variance in the dependent variable that can be explained by the independent variable. In other words,\\nr-squared shows how well the data fit the regression model (the goodness of fit).\\nR2 = 1-(SSresidual/SStotal)\\nwhere R2 is the r squared\\n      SSresidual is the sum of squares of the difference between the actual and the predicted values of the dependent variable\\n      SStotal is the sum of the squares of the difference between the actual and the mean of the dependent value.\\nR-squared shows how well the data fit the regression model (the goodness of fit).\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "-> R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines \n",
    "the proportion of variance in the dependent variable that can be explained by the independent variable. In other words,\n",
    "r-squared shows how well the data fit the regression model (the goodness of fit).\n",
    "R2 = 1-(SSresidual/SStotal)\n",
    "where R2 is the r squared\n",
    "      SSresidual is the sum of squares of the difference between the actual and the predicted values of the dependent variable\n",
    "      SStotal is the sum of the squares of the difference between the actual and the mean of the dependent value.\n",
    "R-squared shows how well the data fit the regression model (the goodness of fit).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57477b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. \\n->Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more \\nthan would be expected by chance. It decreases when a predictor improves the model by less than expected. \\nEvery time we add a independent variable to a model, the R-squared increases, even if the independent variable is insignificant. It never declines. Whereas Adjusted R-squared increases only when \\nindependent variable is significant and affects dependent variable.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. \n",
    "->Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more \n",
    "than would be expected by chance. It decreases when a predictor improves the model by less than expected. \n",
    "Every time we add a independent variable to a model, the R-squared increases, even if the independent variable is insignificant. It never declines. Whereas Adjusted R-squared increases only when \n",
    "independent variable is significant and affects dependent variable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c90a043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q3. When is it more appropriate to use adjusted R-squared?\\n->It is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q3. When is it more appropriate to use adjusted R-squared?\n",
    "->It is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c16d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\\ncalculated, and what do they represent?\\n-> RMSE: Root mean squared error. It gives us the square root of the sum of the squares of the difference between the actual and the predicted value\\nof the dependent variable.\\nMSE: Mean squared error. It gives us the sum of the squares of the difference between the actual and the predicted value\\nof the dependent variable.\\nMAE: Mean Absolute Error. It gives us the sum of the absolute difference between the actual and the predicted value of the dependent variable.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "-> RMSE: Root mean squared error. It gives us the square root of the sum of the squares of the difference between the actual and the predicted value\n",
    "of the dependent variable.\n",
    "MSE: Mean squared error. It gives us the sum of the squares of the difference between the actual and the predicted value\n",
    "of the dependent variable.\n",
    "MAE: Mean Absolute Error. It gives us the sum of the absolute difference between the actual and the predicted value of the dependent variable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a21487dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\\nregression analysis.\\n->RMSE: \\nAdvantage: Treats small differences between small actual and predicted values same as big differences between large actual and predicted values.\\nDisadvantage: Penalizes underestimates more than the overestimates.\\n\\n->MSE: \\nAdvantage: The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts larger weight on these \\nerrors due to the squaring part of the function.\\nDisadvantage: If our model makes a single very bad prediction, the squaring part of the function magnifies the error.\\n\\n->MAE:\\nAdvantage: It is not subject to variations in the distribution of error magnitude and sample size\\nDisadvantage: It may causes convergence problems.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "->RMSE: \n",
    "Advantage: Treats small differences between small actual and predicted values same as big differences between large actual and predicted values.\n",
    "Disadvantage: Penalizes underestimates more than the overestimates.\n",
    "\n",
    "->MSE: \n",
    "Advantage: The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts larger weight on these \n",
    "errors due to the squaring part of the function.\n",
    "Disadvantage: If our model makes a single very bad prediction, the squaring part of the function magnifies the error.\n",
    "\n",
    "->MAE:\n",
    "Advantage: It is not subject to variations in the distribution of error magnitude and sample size\n",
    "Disadvantage: It may causes convergence problems.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3652b577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\\nit more appropriate to use?\\n-> The LASSO method regularizes model parameters by shrinking the regression coefficients, reducing some of them to zero.\\nLasso regression, commonly referred to as L1 regularization, is a method for stopping overfitting in linear regression models \\nby including a penalty term in the cost function. In contrast to Ridge regression, it adds the total of the absolute values \\nof the coefficients rather than the sum of the squared coefficients.\\nIt is more relevent to use Lasso regularization when some of the features in the dataset are irrelevent or redundant.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "-> The LASSO method regularizes model parameters by shrinking the regression coefficients, reducing some of them to zero.\n",
    "Lasso regression, commonly referred to as L1 regularization, is a method for stopping overfitting in linear regression models \n",
    "by including a penalty term in the cost function. In contrast to Ridge regression, it adds the total of the absolute values \n",
    "of the coefficients rather than the sum of the squared coefficients.\n",
    "It is more relevent to use Lasso regularization when some of the features in the dataset are irrelevent or redundant.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897c16e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q7. How do regularized linear models help to prevent overfitting in machine learning?\\n-> Overfitting is a phenomenon where a machine learning model models the training data too well but fails to perform well on the testing data.\\nRegularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates \\ntowards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q7. How do regularized linear models help to prevent overfitting in machine learning?\n",
    "-> Overfitting is a phenomenon where a machine learning model models the training data too well but fails to perform well on the testing data.\n",
    "Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates \n",
    "towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526ff57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\\nchoice for regression analysis.\\n-> Regularization leads to dimensionality reduction, which means the machine learning model is built using a lower dimensional dataset. This generally leads to a high bias error.\\nIf regularization is performed before training the model, a perfect balance between bias-variance tradeoff must be used.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "-> Regularization leads to dimensionality reduction, which means the machine learning model is built using a lower dimensional dataset. This generally leads to a high bias error.\n",
    "If regularization is performed before training the model, a perfect balance between bias-variance tradeoff must be used.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1d084b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q9. You are comparing the performance of two regression models using different evaluation metrics.\\nModel A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\\nperformer, and why? Are there any limitations to your choice of metric?\\n-> Model A has a lower RMSE (10), indicating that it might perform better when large errors are of particular concern.\\nModel B has a lower MAE (8), suggesting that it might perform better in terms of overall average prediction accuracy.\\nRMSE and MAE are in different units than the original variable. Comparing models based solely on these metrics might \\nnot provide the full picture if the scales of the variables are different.\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "-> Model A has a lower RMSE (10), indicating that it might perform better when large errors are of particular concern.\n",
    "Model B has a lower MAE (8), suggesting that it might perform better in terms of overall average prediction accuracy.\n",
    "RMSE and MAE are in different units than the original variable. Comparing models based solely on these metrics might \n",
    "not provide the full picture if the scales of the variables are different.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "653dd30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q10. You are comparing the performance of two regularized linear models using different types of\\nregularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\\nuses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\\nbetter performer, and why? Are there any trade-offs or limitations to your choice of regularization\\nmethod?\\n-> The choice of model will depend on hoe relevent the features are to the model and also on the presence \\nof outliers. While lasso regularization can be preferred in case of more redundant features, ridge \\nregularization is more preferrable if all the features are of equal importance. Also both models cannot\\nbe used on more complex models as they are inflexible and only try to avoid the risk of overfitting in the \\nmodels.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "-> The choice of model will depend on hoe relevent the features are to the model and also on the presence \n",
    "of outliers. While lasso regularization can be preferred in case of more redundant features, ridge \n",
    "regularization is more preferrable if all the features are of equal importance. Also both models cannot\n",
    "be used on more complex models as they are inflexible and only try to avoid the risk of overfitting in the \n",
    "models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d79b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
